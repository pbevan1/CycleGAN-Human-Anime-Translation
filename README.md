![image](https://user-images.githubusercontent.com/69802263/109550115-c4df1680-7ac6-11eb-81a0-8cbee06ade02.png)

# CycleGAN-Human-Anime Translation
This project aimed to transfer between human and anime faces using an implementation of the CycleGAN model in Keras with Tensorflow backend. The model is mostly taken from machinelearningmastery.com with some tweaks, namely switching out the discriminator for one taken (with permission) from code by https://github.com/nanoxas, as well as manually decaying the learning rate. Many runs of 50,000+ passes led to either mode collapse or very poor results. Adjusting the anime images to be at a similar scale to the human faces (human training images were more zoomed in) seemed to allow the model to map facial features better and build from there. Further to this the results much improved with a discriminator with less strided convolutions and a dense layer. This could be due to the original model being prepared for 256x256 images whereas the training images in this project were fed into the model as 64x64 (too many strides were maybe down sampling too much). Once the results plateaued at around 100,000 passes, the learning rate was decayed manually, as described by Jun-Yan Zhu et al. (2017) in the original paper, from 0.0002 to 0.0001. This seemed to add a little detail and clarity to the output. For further projects using CycleGAN I would maybe try and continue the decay linearly to 0 as the paper specifies, in an automated manner rather than manually. After a couple of bad experiences with the notebook crashing, the code was adapted to save the weights and plots of the output to google drive every 5000 passes, allowing the outputs to be examined and the best weights chosen either for continued training or as the final weights. An attempt was made to make sure the work is reproducible, by documenting the process, giving downloadable links to the data used in the notebook and setting random seeds.

The final results were quite pleasing, especially after the initial disappointing results, and can be seen below. While some outputs are more convincing than others, the model seems to consistently output realistic anime characters that are recognisable as the input. The model sometimes struggles to pick up features when there is a lack of contrast in the images, for example sometimes eyes are not mapped well. Some images do not have clear mouths in the output, although many of the anime training images have small or non-existent mouths so this is likely a factor. The model also struggles slightly with glasses but has managed ok on some of the examples below. The ability to deal with accessories like this was still visibly improving with training, so further training could lead to better results in this regard. Itâ€™s clear from the images below that the vast majority of the anime training images have white skin, since the learned distribution transforms all skin tones to be white. This highlights a possible ethical concern if these models were to be deployed. The within sample results are a lot more consistent (also cherry picked) in comparison to the out of sample results, but the model still generalises well. The recreation of the human faces from the anime faces are not convincingly human but are usually recognisable as the original person. Black dead spots appear sometimes for unknown reasons, this would be interesting to investigate further.
